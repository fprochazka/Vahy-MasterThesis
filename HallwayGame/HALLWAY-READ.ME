Hallway: a classic probabilistic planning robot navigation benchmark. We have a maze with walls, traps and goals.  The state of the robot is determined by its coordinates in the grid and by its heading (N,W,S,E).
In each step, the robot can move forward or turn left and right. In the POMDP version, it can only sense the presence of walls in neigbouring cells (it cannot
sense traps). The goal is to find the target as fast as possible (i.e. each action gets a small penalty, say -1, until a target is reached, at which point a big reward, say 100 or 1000, is received). Once the goal is reached, we enter a dummy sink state where reward zero is received in each step.

There are two types of probabilistic behaviour: first, the starting cell can be chosen randomly from a set of specified cells. Next, whenever the robot makes a move forward, there is a small chance (say 0.1), that it instead moves diagonally forward, i.e. with probability 0.05 to the cell diagonally to the left in fron of him and with prob 0.05 diagonally to the right. If a robot tries to enter a cell with a wall, it just bumps into it and stays in the same cell.

There are also trap cells in the maze: whenever the robot walks into the trap, it gets destroyed.

This is just a basic variant, which can be enriched by additional features. For instance, we can have multiple goal states: these then represent mines that can be mined for a reward. We can visit multiple mines, and only enter the final state if all mine has been visited. However, each mine can be mined at most once, so the state of the maze is now described by the position of the robot + a set of binary flags, one per mine, which determines if the mine was mined already or not. This blows-up the state space exponentially in the number of mines. Also, it can be used to introduce more risk into the game, since there might be an area with a lot of rich mines, but getting there might require navigating around lots of traps etc.

Example maze:

1 1 1 1 1 1
1 0 + + 0 1
1 1 + + 1 1
1 0 0 0 0 1
1 0 g x 0 1
1 1 1 1 1 1

1s are walls, 0s are empty cells, g is a goal state, x is a trap  + is a possible starting location. The robot starts in a starting location
chosen uniformly at random, looking (say) southwards.


In case where there are multiple goals, we may opt to have different types of goals (mines) with different rewards. This nicely introduces the notion of a risk, since maybe it is risky to get to higher-reward mines. In such a case, we might denote different type of goals by various numbers >1.

(Mine 3 has higher reward than mine 2):

1 1 1 1 1 1 1
1 + x 0 x 3 1
1 1 1 2 1 1 1
1 1 1 1 1 1 1

Another option to incorporate risk to the gridwold domain is that we want to bound the risk that it takes too much time to reach the goal. In such a case, states can be extended with a variable counting the elapsed time and traps simply delay the robot.
